{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project in Keras\n",
    "\n",
    "Based on: https://github.com/rachhek/speech_recognition_using_lstm/blob/master/speech_recognition_using_lstm_experiment.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python_speech_features\n",
      "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: python_speech_features\n",
      "  Building wheel for python_speech_features (setup.py): started\n",
      "  Building wheel for python_speech_features (setup.py): finished with status 'done'\n",
      "  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5879 sha256=d4671398ce46a9806edd386b73f90aa5846d9a18f9b3781d67c79cec2dfca4c0\n",
      "  Stored in directory: c:\\users\\48695\\appdata\\local\\pip\\cache\\wheels\\37\\01\\19\\e6c69a32684ab7b2e3ea4985a571d810cf055c72600e7f9f17\n",
      "Successfully built python_speech_features\n",
      "Installing collected packages: python_speech_features\n",
      "Successfully installed python_speech_features-0.6\n"
     ]
    }
   ],
   "source": [
    "! pip install python_speech_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional,Flatten,Input\n",
    "from keras.layers import SpatialDropout1D, SpatialDropout2D, SpatialDropout3D, Bidirectional\n",
    "from keras.layers import Conv1D, BatchNormalization, Conv2D, MaxPooling2D, MaxPooling1D, Flatten, Dropout\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, GlobalAveragePooling2D, Layer\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder,normalize\n",
    "from matplotlib import pyplot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pickle \n",
    "\n",
    "metrics=['categorical_accuracy','AUC','Precision','Recall'] #można potem obliczyć F1 na podstawie recall i precision\n",
    "metrics2 = ['categorical_accuracy','precision','recall']\n",
    "train_path = \"./train/audio/\" \n",
    "val_text = \"./train/validation_list.txt\"\n",
    "test_text = \"./train/testing_list.txt\"\n",
    "root = \"./train/\"\n",
    "preprocessed = \"./preprocessed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(model, test_ds, num_classes, class_names):\n",
    "    true_labels = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "    tf_labels = tf.argmax(true_labels, axis=1).numpy()\n",
    "\n",
    "    predictions = model.predict(test_ds)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    conf_matrix = tf.math.confusion_matrix(tf_labels, predicted_classes, num_classes=num_classes)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix.numpy(), display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(11, 9))  # Ustaw rozmiar wykresu na 10x8\n",
    "\n",
    "    # Wyświetl macierz pomyłek z określonym rozmiarem\n",
    "    disp.plot(ax=ax)  # Użyj parametru ax, aby użyć określonej osi\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_training_validation_loss_and_metrics(model,metrics):\n",
    "    f,ax=plt.subplots(2,1,figsize=(20,20)) \n",
    "\n",
    "    #Assigning the first subplot to graph training loss and validation loss\n",
    "    ax[0].plot(model.history.history['loss'],color='b',label='Training Loss')\n",
    "    ax[0].plot(model.history.history['val_loss'],color='r',label='Validation Loss')\n",
    "\n",
    "    #Plotting the training accuracy and validation accuracy\n",
    "    for metric in metrics:\n",
    "        ax[1].plot(model.history.history[metric],label='Training '+metric)\n",
    "        ax[1].plot(model.history.history['val_'+metric],label='Validation '+metric)\n",
    "    \n",
    "    plt.legend() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing with spectogram (can be run only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_background_noise(root_path='./train', input_folder='_background_noise_', output_folder='silence'):\n",
    "    audio_path = os.path.join(root_path, 'audio')\n",
    "    input_path = os.path.join(audio_path, input_folder)\n",
    "    output_path = os.path.join(audio_path, output_folder)\n",
    "    \n",
    "    sample_rate = 16000\n",
    "    sample_length = 1\n",
    "\n",
    "    audio_files = [d for d in os.listdir(input_path)\n",
    "                   if os.path.isfile(os.path.join(input_path, d)) and d.endswith('.wav')]\n",
    "    samples = []\n",
    "\n",
    "    for f in audio_files:\n",
    "        path = os.path.join(input_path, f)\n",
    "        s, _ = librosa.load(path, sr=sample_rate)\n",
    "        samples.append(s)\n",
    "\n",
    "    samples = np.hstack(samples)\n",
    "    c = int(sample_rate * sample_length)\n",
    "    r = len(samples) // c\n",
    "    names = [f'recording_{i}.wav' for i in range(r-1)]\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    for i in range(r - 1):\n",
    "        y = samples[c*i:c*(i+1)]\n",
    "        sf.write(os.path.join(output_path, names[i]), y, sample_rate)\n",
    "\n",
    "    val_choice = np.random.choice(names, int(0.1*len(names)), replace=False).tolist()\n",
    "    with open(os.path.join(root_path, 'validation_list.txt'), 'a') as f:\n",
    "        for name in val_choice:\n",
    "            p = os.path.join(output_folder, name)\n",
    "            p = p.replace('./', '')\n",
    "            f.write(p)\n",
    "            f.write('\\n')\n",
    "\n",
    "    test_choice = np.random.choice([n for n in names if n not in val_choice], int(0.1*len(names)), replace=False).tolist()\n",
    "    with open(os.path.join(root_path, 'testing_list.txt'), 'a') as f:\n",
    "        for name in test_choice:\n",
    "            p = os.path.join(output_folder, name)\n",
    "            p = p.replace('./', '')\n",
    "            f.write(p)\n",
    "            f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_background_noise(root_path=root, input_folder='_background_noise_', output_folder='silence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(root_path_files, files_names_list_name, label_encoder=None):\n",
    "    #Calculating x_test and y_test        \n",
    "    test_labels = []\n",
    "    test_data = []\n",
    "\n",
    "    #test_labels.txt is a txt file with all labels for the speech samples that is required for the evaluation. We loop through it to calculate the MFCC value for each speech sample and then normalize it\n",
    "    with open(os.path.join(root_path_files, files_names_list_name), newline='') as tsvfile:\n",
    "        reader = csv.DictReader(tsvfile)\n",
    "        reader = csv.reader(tsvfile, delimiter=' ')\n",
    "        for row in reader:\n",
    "            wav_file = os.path.join(root_path_files, \"audio/\", row[0])\n",
    "\n",
    "            row.append(row[0].split(\"/\")[0])\n",
    "            (rate,sig) = wav.read(wav_file)\n",
    "\n",
    "            # pad to 1s of length using pad_sequences\n",
    "            sig = pad_sequences([sig], maxlen=16000, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "            #Getting the MFCC value from the .wav files.\n",
    "            mfcc_feat = mfcc(sig,rate)\n",
    "            \n",
    "            scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            scaler = scaler.fit(mfcc_feat)\n",
    "\n",
    "            #Normalizing the MFCC values.\n",
    "            normalized = scaler.transform(mfcc_feat)\n",
    "            test_data.append(normalized)\n",
    "            test_labels.append(str(row[1]))\n",
    "        \n",
    "        if label_encoder is None:\n",
    "            label_encoder_test = LabelEncoder().fit(test_labels)\n",
    "        else:\n",
    "            label_encoder_test = label_encoder\n",
    "        vec_test = label_encoder_test.transform(test_labels)\n",
    "\n",
    "        #One hot encoding the labels\n",
    "        one_hot_labels_test = keras.utils.to_categorical(vec_test, num_classes=len(label_encoder_test.classes_))\n",
    "        Y_test = one_hot_labels_test\n",
    "        X_test = np.array(test_data,dtype=np.float32)\n",
    "        return X_test, Y_test, label_encoder_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_txt(root_path_files, files_names_list_name):\n",
    "    omit = []\n",
    "    train = []\n",
    "    for f in files_names_list_name:\n",
    "        with open(os.path.join(root_path_files, f)) as fileobj:\n",
    "            omit += [line.strip() for line in fileobj]\n",
    "    for target in os.listdir(os.path.join(root_path_files, 'audio')):\n",
    "        if not target.startswith('_'):\n",
    "            for file in os.listdir(os.path.join(root_path_files, 'audio', target)):\n",
    "                p = os.path.join(target, file)\n",
    "                p = p.replace(\"\\\\\",\"/\")\n",
    "                if p not in omit:\n",
    "                    train.append(p)\n",
    "    with open(os.path.join(root_path_files, 'training_list.txt'), 'wb') as file:\n",
    "        for t in train:\n",
    "            file.write(t.encode())\n",
    "            file.write('\\n'.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'silence', 'six', 'stop', 'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero', '_background_noise_']\n"
     ]
    }
   ],
   "source": [
    "root_path_files = root\n",
    "root_saved_files = preprocessed\n",
    "generate_train_txt(root_path_files, ['validation_list.txt', 'testing_list.txt'])\n",
    "classes = os.listdir(root_path_files+\"/audio\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path_files = root\n",
    "root_saved_files = preprocessed\n",
    "\n",
    "generate_train_txt(root_path_files, ['validation_list.txt', 'testing_list.txt'])\n",
    "\n",
    "train_files_names_list_name = 'training_list.txt'\n",
    "X_train, Y_train, label_encoder = load_dataset(root_path_files=root_path_files, files_names_list_name=train_files_names_list_name)\n",
    "np.save(os.path.join(root_saved_files, 'X_train'), X_train)\n",
    "np.save(os.path.join(root_saved_files, 'Y_train'), Y_train)\n",
    "\n",
    "valid_files_names_list_name = 'validation_list.txt'\n",
    "X_valid, Y_valid, _ = load_dataset(root_path_files=root_path_files, files_names_list_name=valid_files_names_list_name, \n",
    "                                   label_encoder=label_encoder)\n",
    "np.save(os.path.join(root_saved_files, 'X_valid'), X_valid)\n",
    "np.save(os.path.join(root_saved_files, 'Y_valid'), Y_valid)\n",
    "\n",
    "test_files_names_list_name = 'testing_list.txt'\n",
    "X_test, Y_test, _ = load_dataset(root_path_files=root_path_files, files_names_list_name=test_files_names_list_name, \n",
    "                                 label_encoder=label_encoder)\n",
    "np.save(os.path.join(root_saved_files, 'X_test'), X_test)\n",
    "np.save(os.path.join(root_saved_files, 'Y_test'), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('encoder', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bed' 'bird' 'cat' 'dog' 'down' 'eight' 'five' 'four' 'go' 'happy'\n",
      " 'house' 'left' 'marvin' 'nine' 'no' 'off' 'on' 'one' 'right' 'seven'\n",
      " 'sheila' 'silence' 'six' 'stop' 'three' 'tree' 'two' 'up' 'wow' 'yes'\n",
      " 'zero']\n"
     ]
    }
   ],
   "source": [
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install gdown\n",
    "#! gdown https://drive.google.com/uc?id=1S0ZWGTnKzyaYfLUOzii_LFnrqdygLDtf\n",
    "#! mkdir ./preprocessed\n",
    "#! unzip preprocessed.zip -d ./preprocessed\n",
    "#! mv ./preprocessed/encoder ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test data and labels\n",
    "root_saved_files = preprocessed\n",
    "\n",
    "X_train = np.load(os.path.join(root_saved_files,'X_train.npy'))\n",
    "Y_train = np.load(os.path.join(root_saved_files, 'Y_train.npy'))\n",
    "\n",
    "X_test = np.load(os.path.join(root_saved_files, 'X_test.npy'))\n",
    "Y_test = np.load(os.path.join(root_saved_files, 'Y_test.npy'))\n",
    "\n",
    "X_valid = np.load(os.path.join(root_saved_files, 'X_valid.npy'))\n",
    "Y_valid = np.load(os.path.join(root_saved_files, 'Y_valid.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('encoder', 'rb') as f:\n",
    "    encoder = pickle.load(f)\n",
    "    CLASSES = encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bed' 'bird' 'cat' 'dog' 'down' 'eight' 'five' 'four' 'go' 'happy'\n",
      " 'house' 'left' 'marvin' 'nine' 'no' 'off' 'on' 'one' 'right' 'seven'\n",
      " 'sheila' 'silence' 'six' 'stop' 'three' 'tree' 'two' 'up' 'wow' 'yes'\n",
      " 'zero']\n"
     ]
    }
   ],
   "source": [
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_seed(new_random_seed):\n",
    "    np.random.seed(new_random_seed)\n",
    "    tf.keras.utils.set_random_seed(new_random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, test_data, lr, epochs, batch, path='checkpoint'):\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3, mode = 'min')\n",
    "    ]\n",
    "    m = model()\n",
    "    m.compile(optimizer=Adam(amsgrad=True, learning_rate=lr),loss='categorical_crossentropy',metrics=metrics)\n",
    "    history = m.fit(train_data[0], train_data[1],\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    batch_size=batch,\n",
    "                    validation_data=val_data,\n",
    "                    verbose=1,\n",
    "                    shuffle=True)\n",
    "\n",
    "    datetime = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    m.save(os.path.join(path, 'model_' + datetime + '.keras'))\n",
    "\n",
    "    show_training_validation_loss_and_metrics(m, metrics2)\n",
    "\n",
    "    y_prediction = m.predict(test_data[0])\n",
    "    y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "    y_test_single_column=np.argmax(test_data[1], axis=1)\n",
    "    result = confusion_matrix(y_test_single_column, y_prediction , normalize='pred')\n",
    "    plt.figure(figsize=(20,20))\n",
    "    labels = CLASSES\n",
    "    sns.heatmap(result, annot=True, fmt='.2f', xticklabels=labels, yticklabels=labels)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion matrix on test data')\n",
    "    plt.show()\n",
    "    \n",
    "    result = confusion_matrix(y_test_single_column, y_prediction)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    labels = CLASSES\n",
    "    sns.heatmap(result, annot=True, fmt='.2f', xticklabels=labels, yticklabels=labels)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion matrix on test data')\n",
    "    plt.show()\n",
    "    \n",
    "    acc_train = accuracy_score(np.argmax(train_data[1], axis=1), np.argmax(m.predict(train_data[0]), axis = 1))\n",
    "    print(f\"Accuracy score on train dataset: {acc_train}\")\n",
    "    acc_val = accuracy_score(np.argmax(val_data[1], axis=1), np.argmax(m.predict(val_data[0]), axis = 1))\n",
    "    print(f\"Accuracy score on validation dataset: {acc_val}\")\n",
    "    acc_test = accuracy_score(y_test_single_column, y_prediction)\n",
    "    print(f\"Accuracy score on test dataset: {acc_test}\")\n",
    "\n",
    "    return [acc_train, acc_val, acc_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_train(model, train_data, val_data, test_data, lr, epochs, batch, seeds, path='checkpoint'):\n",
    "    accuracy = []\n",
    "    for seed in seeds:\n",
    "        print(f\"Training with seed {seed}\")\n",
    "        p = os.path.join(path, str(seed))\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        if not os.path.exists(p):\n",
    "            os.mkdir(p)\n",
    "        update_seed(seed)\n",
    "        acc = train_model(model, train_data, val_data, test_data, lr, epochs, batch, path=p)\n",
    "        accuracy.append(acc)\n",
    "    with open(os.path.join(path, 'accuracy'), 'wb') as f:\n",
    "        pickle.dump(accuracy, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelLSTM():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200,input_shape=(99,13),return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(Y_test.shape[1], activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    \n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    context, x  = inputs\n",
    "\n",
    "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, latent_dim,):\n",
    "        super().__init__()\n",
    "        self.att_1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.att_2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(latent_dim, activation=\"relu\"), Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.reshape = keras.layers.Reshape((1,32))\n",
    "\n",
    "    def call(self, inputs, encoder_outputs):\n",
    "        attn_output_1 = self.att_1(inputs, inputs)\n",
    "        out1 = self.layernorm1(inputs + attn_output_1)\n",
    "\n",
    "        encoder_outputs = self.reshape(encoder_outputs)\n",
    "        attn_output_2 = self.att_2(out1, encoder_outputs)\n",
    "        out2 = self.layernorm2(out1 + attn_output_2)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        return self.layernorm3(out2 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTransformer():\n",
    "    embedding_size=32\n",
    "    num_attn_heads=2\n",
    "    ff_net_dim=32\n",
    "    maxlen=99 #chyba?\n",
    "    vocab_size=13 #chyba?\n",
    "    input_layer = keras.Input(shape=(99,13))\n",
    "    x = TokenAndPositionEmbedding(maxlen, vocab_size, embedding_size)(input_layer)\n",
    "    x = TransformerBlock(embedding_size, num_attn_heads, ff_net_dim)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(20, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    output_layer = Dense(Y_test.shape[1], activation='softmax')(x)\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTransformer_gpt():\n",
    "    embedding_size=128\n",
    "    num_attn_heads=8\n",
    "    ff_net_dim=128\n",
    "    maxlen=1287 # before 99\n",
    "    vocab_size= n_classes # before 13\n",
    "\n",
    "    # define input layers\n",
    "    encoder_input_layer = keras.Input(shape=(99*13,))\n",
    "    decoder_input_layer = keras.Input(shape=(99*13,))\n",
    "\n",
    "    # Encoder\n",
    "    x_enc = TokenAndPositionEmbedding(maxlen, vocab_size, embedding_size)(encoder_input_layer)\n",
    "    x_enc = TransformerEncoder(embedding_size, num_attn_heads, ff_net_dim)(x_enc)\n",
    "    x_enc = GlobalAveragePooling1D()(x_enc)\n",
    "    x_enc = Dropout(0.1)(x_enc)\n",
    "\n",
    "    # Decoder\n",
    "    x_dec = TokenAndPositionEmbedding(maxlen, vocab_size, embedding_size)(decoder_input_layer)\n",
    "    x_dec = TransformerDecoder(embedding_size, num_attn_heads, ff_net_dim)(x_dec, x_enc)\n",
    "    x_dec = GlobalAveragePooling1D()(x_dec)\n",
    "    x_dec = Dropout(0.1)(x_dec)\n",
    "\n",
    "    # Concat\n",
    "    combined_output = keras.layers.Concatenate()([x_enc, x_dec])\n",
    "\n",
    "    # last dense\n",
    "    x = Dense(20, activation='relu')(combined_output)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    # output layer\n",
    "    output_layer = Dense(Y_test.shape[1], activation='softmax')(x)\n",
    "    model = keras.Model(inputs=[encoder_input_layer, decoder_input_layer], outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51486, 99, 13)\n",
      "(51486, 31)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (51486, 99, 13)\n",
      "Shape of X_train reshaped: (51486, 1287)\n"
     ]
    }
   ],
   "source": [
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 99*13))\n",
    "\n",
    "# Check the shape of the reshaped data\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_train reshaped:\", X_train_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_49\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_49\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_52      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1287</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_and_position… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1287</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,600</span> │ input_layer_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1287</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,656</span> │ token_and_positi… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_enco… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_53      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1287</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_106         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_and_position… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1287</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,600</span> │ input_layer_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_decode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1287</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,136</span> │ token_and_positi… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ dropout_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_deco… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_109         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_109[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_96 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,300</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_110         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_96[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_97 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">651</span> │ dropout_110[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_52      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1287\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_and_position… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1287\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m41,600\u001b[0m │ input_layer_52[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mTokenAndPositionE…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1287\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m10,656\u001b[0m │ token_and_positi… │\n",
       "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ transformer_enco… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_53      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1287\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_106         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_and_position… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1287\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m41,600\u001b[0m │ input_layer_53[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mTokenAndPositionE…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_decode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1287\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m19,136\u001b[0m │ token_and_positi… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ dropout_106[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ transformer_deco… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_109         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_106[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_109[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_96 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │      \u001b[38;5;34m1,300\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_110         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_96[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_97 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m)        │        \u001b[38;5;34m651\u001b[0m │ dropout_110[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">114,943</span> (449.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m114,943\u001b[0m (449.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">114,943</span> (449.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m114,943\u001b[0m (449.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = modelTransformer_gpt()\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m1609/1609\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0363 - loss: 3.4289"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer 'functional_49' expected 2 input(s). Received 1 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      2\u001b[0m     [X_train_reshaped, X_train_reshaped], Y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_valid, Y_valid)\n\u001b[0;32m      3\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:156\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_spec) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs):\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# Having a shape/dtype is the only commonality of the various\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# tensor-like objects that may be passed. The most common kind of\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# invalid type we are guarding for is a Layer instance (Functional API),\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# which does not have a `shape` attribute.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Layer 'functional_49' expected 2 input(s). Received 1 instead."
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train_reshaped, X_train_reshaped], Y_train, batch_size=32, epochs=2, validation_data=(X_valid, Y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_models as tfm\n",
    "from tensorflow_models.nlp import layers\n",
    "\n",
    "print('worked?')\n",
    "#tfm.nlp.layers.TransformerEncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTransformerLib_enc():\n",
    "    inputs = tf.keras.Input(shape=(28,10,))\n",
    "    x_encoded = tfm.nlp.layers.TransformerEncoderBlock(10,32,'relu')(inputs)\n",
    "    #x_decoded = tfm.nlp.layers.TransformerDecoderBlock(9,32,'relu', input_shape=(99,13,))(inputs)\n",
    "    outputs = tf.keras.layers.Dense(20, activation='softmax')(x_encoded) #Y_test.shape[1]\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from official.nlp.modeling.models import TransformerEncoder, TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTransformerLib_enc2():\n",
    "    inputs = tf.keras.Input(shape=(99,13,))\n",
    "    x_encoded = official.nlp.modeling.models.TransformerEncoder(num_attention_heads=13)(inputs)\n",
    "    x_decoded = official.nlp.modeling.models.TransformerDecoder(num_attention_heads=13)(inputs, x_encoded)\n",
    "    outputs = tf.keras.layers.Dense(20, activation='softmax')(x_encoded) #Y_test.shape[1]\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'official' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodelTransformerLib_enc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[89], line 3\u001b[0m, in \u001b[0;36mmodelTransformerLib_enc2\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodelTransformerLib_enc2\u001b[39m():\n\u001b[0;32m      2\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m99\u001b[39m,\u001b[38;5;241m13\u001b[39m,))\n\u001b[1;32m----> 3\u001b[0m     x_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mofficial\u001b[49m\u001b[38;5;241m.\u001b[39mnlp\u001b[38;5;241m.\u001b[39mmodeling\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mTransformerEncoder(num_attention_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m)(inputs)\n\u001b[0;32m      4\u001b[0m     x_decoded \u001b[38;5;241m=\u001b[39m official\u001b[38;5;241m.\u001b[39mnlp\u001b[38;5;241m.\u001b[39mmodeling\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mTransformerDecoder(num_attention_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m)(inputs, x_encoded)\n\u001b[0;32m      5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m20\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)(x_encoded) \u001b[38;5;66;03m#Y_test.shape[1]\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'official' is not defined"
     ]
    }
   ],
   "source": [
    "model = modelTransformerLib_enc2()\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTransformerLib():\n",
    "    inputs = tf.keras.Input(shape=(28,10,))\n",
    "    x_encoded = tfm.nlp.layers.TransformerEncoderBlock(10,32,'relu')(inputs)\n",
    "    input_shape_tmp=(28,10)\n",
    "    print(tf.TensorShape(input_shape_tmp[0]).as_list())\n",
    "    x_decoded = tfm.nlp.layers.TransformerDecoderBlock(13,32,'relu')(inputs, x_encoded)\n",
    "    outputs = tf.keras.layers.Dense(20, activation='softmax')(x_decoded) #Y_test.shape[1]\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "as_list() is not defined on an unknown TensorShape.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodelTransformerLib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[87], line 6\u001b[0m, in \u001b[0;36mmodelTransformerLib\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m input_shape_tmp\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39mTensorShape(input_shape_tmp[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mas_list())\n\u001b[1;32m----> 6\u001b[0m x_decoded \u001b[38;5;241m=\u001b[39m \u001b[43mtfm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransformerDecoderBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m20\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)(x_decoded) \u001b[38;5;66;03m#Y_test.shape[1]\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\plswork\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\plswork\\lib\\site-packages\\official\\nlp\\modeling\\layers\\transformer.py:239\u001b[0m, in \u001b[0;36mTransformerDecoderBlock.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape):\n\u001b[0;32m    238\u001b[0m   target_tensor_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensorShape(input_shape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 239\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtarget_tensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformerLayer expects a three-dimensional input of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape [batch, sequence, width].\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    242\u001b[0m   hidden_size \u001b[38;5;241m=\u001b[39m target_tensor_shape[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape."
     ]
    }
   ],
   "source": [
    "model = modelTransformerLib()\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTransformer2():\n",
    "    embed_dim=32\n",
    "    num_heads=2\n",
    "    ff_dim=32\n",
    "    maxlen=99 #chyba?\n",
    "    vocab_size=n_classes #chyba?\n",
    "    \n",
    "    input_layer = keras.Input(shape=(99,13))\n",
    "    x = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)(input_layer)\n",
    "    encoder_outputs = TransformerEncoder(embed_dim, num_heads, ff_dim)(x)\n",
    "    encoder = keras.Model(input_layer, encoder_outputs)\n",
    "    \n",
    "    #decoder_inputs = keras.Input(shape=(maxlen,), name=\"decoder_inputs\")\n",
    "    encoded_seq_inputs = keras.Input(shape=(None, embed_dim))\n",
    "    x = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)(input_layer)\n",
    "    x = TransformerDecoder(embed_dim, ff_dim, num_heads)(x, encoded_seq_inputs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    decoder_outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    decoder = keras.Model([input_layer, encoded_seq_inputs], decoder_outputs)\n",
    "    \n",
    "    decoder_outputs = decoder([input_layer, encoder_outputs])\n",
    "\n",
    "    #output_layer = Dense(Y_test.shape[1], activation='softmax')(x)\n",
    "    #model = keras.Model(inputs=inputs, outputs=decoder_outputs)\n",
    "    transformer = keras.Model(input_layer, decoder_outputs)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48695\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:1285: UserWarning: Layer 'transformer_decoder_4' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Exception encountered when calling MultiHeadAttention.call().\n",
      "\n",
      "\u001b[1mDimension must be 5 but is 4 for '{{node multi_head_attention_36_1/transpose_1}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](multi_head_attention_36_1/Mul, multi_head_attention_36_1/transpose_1/perm)' with input shapes: [?,99,13,2,32], [4].\u001b[0m\n",
      "\n",
      "Arguments received by MultiHeadAttention.call():\n",
      "  • query=tf.Tensor(shape=(None, 99, 13, 32), dtype=float32)\n",
      "  • value=tf.Tensor(shape=(None, None, 32), dtype=float32)\n",
      "  • key=None\n",
      "  • query_mask=None\n",
      "  • value_mask=None\n",
      "  • key_mask=None\n",
      "  • attention_mask=None\n",
      "  • return_attention_scores=False\n",
      "  • training=None\n",
      "  • use_causal_mask=False''\n",
      "  warnings.warn(\n",
      "C:\\Users\\48695\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:360: UserWarning: `build()` was called on layer 'transformer_decoder_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception encountered when calling TransformerDecoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'transformer_decoder_4' (of type TransformerDecoder). Either the `TransformerDecoder.call()` method is incorrect, or you need to implement the `TransformerDecoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling MultiHeadAttention.call().\n\n\u001b[1mDimension must be 5 but is 4 for '{{node multi_head_attention_36_1/transpose_1}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](multi_head_attention_36_1/Mul, multi_head_attention_36_1/transpose_1/perm)' with input shapes: [?,99,13,2,32], [4].\u001b[0m\n\nArguments received by MultiHeadAttention.call():\n  • query=tf.Tensor(shape=(None, 99, 13, 32), dtype=float32)\n  • value=tf.Tensor(shape=(None, None, 32), dtype=float32)\n  • key=None\n  • query_mask=None\n  • value_mask=None\n  • key_mask=None\n  • attention_mask=None\n  • return_attention_scores=False\n  • training=None\n  • use_causal_mask=False\u001b[0m\n\nArguments received by TransformerDecoder.call():\n  • args=('<KerasTensor shape=(None, 99, 13, 32), dtype=float32, sparse=False, name=keras_tensor_133>', '<KerasTensor shape=(None, None, 32), dtype=float32, sparse=None, name=keras_tensor_131>')\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m transformer \u001b[38;5;241m=\u001b[39m modelTransformer2()\n\u001b[0;32m      2\u001b[0m transformer\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m history \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      5\u001b[0m     X_train, Y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_valid, Y_valid)\n\u001b[0;32m      6\u001b[0m )\n",
      "Cell \u001b[1;32mIn[51], line 16\u001b[0m, in \u001b[0;36mmodelTransformer2\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m encoded_seq_inputs \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, embed_dim))\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)(input_layer)\n\u001b[1;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m TransformerDecoder(embed_dim, ff_dim, num_heads)(x, encoded_seq_inputs)\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.5\u001b[39m)(x)\n\u001b[0;32m     18\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m Dense(vocab_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[36], line 17\u001b[0m, in \u001b[0;36mTransformerDecoder.call\u001b[1;34m(self, inputs, encoder_outputs)\u001b[0m\n\u001b[0;32m     14\u001b[0m attn_output_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_1(inputs, inputs)\n\u001b[0;32m     15\u001b[0m out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm1(inputs \u001b[38;5;241m+\u001b[39m attn_output_1)\n\u001b[1;32m---> 17\u001b[0m attn_output_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_2(out1, encoder_outputs)\n\u001b[0;32m     18\u001b[0m out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm2(out1 \u001b[38;5;241m+\u001b[39m attn_output_2)\n\u001b[0;32m     20\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(out2)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Exception encountered when calling TransformerDecoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'transformer_decoder_4' (of type TransformerDecoder). Either the `TransformerDecoder.call()` method is incorrect, or you need to implement the `TransformerDecoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling MultiHeadAttention.call().\n\n\u001b[1mDimension must be 5 but is 4 for '{{node multi_head_attention_36_1/transpose_1}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](multi_head_attention_36_1/Mul, multi_head_attention_36_1/transpose_1/perm)' with input shapes: [?,99,13,2,32], [4].\u001b[0m\n\nArguments received by MultiHeadAttention.call():\n  • query=tf.Tensor(shape=(None, 99, 13, 32), dtype=float32)\n  • value=tf.Tensor(shape=(None, None, 32), dtype=float32)\n  • key=None\n  • query_mask=None\n  • value_mask=None\n  • key_mask=None\n  • attention_mask=None\n  • return_attention_scores=False\n  • training=None\n  • use_causal_mask=False\u001b[0m\n\nArguments received by TransformerDecoder.call():\n  • args=('<KerasTensor shape=(None, 99, 13, 32), dtype=float32, sparse=False, name=keras_tensor_133>', '<KerasTensor shape=(None, None, 32), dtype=float32, sparse=None, name=keras_tensor_131>')\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "transformer = modelTransformer2()\n",
    "transformer.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = transformer.fit(\n",
    "    X_train, Y_train, batch_size=32, epochs=2, validation_data=(X_valid, Y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with seed 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m repeat_train(modelTransformer2(), (X_train, Y_train), (X_valid, Y_valid), (X_test, Y_test), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      2\u001b[0m              seeds\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m], path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_transformer_one_layer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m, in \u001b[0;36mrepeat_train\u001b[1;34m(model, train_data, val_data, test_data, lr, epochs, batch, seeds, path)\u001b[0m\n\u001b[0;32m      9\u001b[0m         os\u001b[38;5;241m.\u001b[39mmkdir(p)\n\u001b[0;32m     10\u001b[0m     update_seed(seed)\n\u001b[1;32m---> 11\u001b[0m     acc \u001b[38;5;241m=\u001b[39m train_model(model, train_data, val_data, test_data, lr, epochs, batch, path\u001b[38;5;241m=\u001b[39mp)\n\u001b[0;32m     12\u001b[0m     accuracy\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_data, val_data, test_data, lr, epochs, batch, path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, train_data, val_data, test_data, lr, epochs, batch, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m         EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     ]\n\u001b[1;32m----> 5\u001b[0m     m \u001b[38;5;241m=\u001b[39m model()\n\u001b[0;32m      6\u001b[0m     m\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(amsgrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, learning_rate\u001b[38;5;241m=\u001b[39mlr),loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m      7\u001b[0m     history \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mfit(train_data[\u001b[38;5;241m0\u001b[39m], train_data[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      8\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m      9\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     13\u001b[0m                     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\inspect.py:3212\u001b[0m, in \u001b[0;36mSignature.bind\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[0;32m   3210\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[0;32m   3211\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bind(args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\inspect.py:3127\u001b[0m, in \u001b[0;36mSignature._bind\u001b[1;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[0;32m   3125\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   3126\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(arg\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m-> 3127\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3129\u001b[0m     \u001b[38;5;66;03m# We have a positional argument to process\u001b[39;00m\n\u001b[0;32m   3130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: missing a required argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "repeat_train(modelTransformer2(), (X_train, Y_train), (X_valid, Y_valid), (X_test, Y_test), lr=0.001, epochs=3, batch=32,\n",
    "             seeds=[0], path='checkpoint_transformer_one_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_2 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_2 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Exception encountered when calling Transformer.call().\n\n\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by Transformer.call():\n  • inputs=tf.Tensor(shape=(None, 99, 13), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m transformer\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     11\u001b[0m transformer\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     12\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[0;32m     14\u001b[0m     metrics\u001b[38;5;241m=\u001b[39mmetrics2)\n\u001b[1;32m---> 16\u001b[0m transformer\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_valid,Y_valid))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[24], line 223\u001b[0m, in \u001b[0;36mTransformer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m    221\u001b[0m   \u001b[38;5;66;03m# To use a Keras model with `.fit` you must pass all your inputs in the\u001b[39;00m\n\u001b[0;32m    222\u001b[0m   \u001b[38;5;66;03m# first argument.\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m   context, x  \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m    225\u001b[0m   context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(context)  \u001b[38;5;66;03m# (batch_size, context_len, d_model)\u001b[39;00m\n\u001b[0;32m    227\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x, context)  \u001b[38;5;66;03m# (batch_size, target_len, d_model)\u001b[39;00m\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling Transformer.call().\n\n\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by Transformer.call():\n  • inputs=tf.Tensor(shape=(None, 99, 13), dtype=float32)"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=2,\n",
    "    d_model=32,\n",
    "    num_heads=2,\n",
    "    dff=64,\n",
    "    input_vocab_size=n_classes,\n",
    "    target_vocab_size=n_classes,\n",
    "    dropout_rate=0.1)\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=metrics2)\n",
    "\n",
    "transformer.fit(X_train, Y_train, batch_size=64, epochs=3, validation_data=(X_valid,Y_valid))\n",
    "#repeat_train(transformer, (X_train, Y_train), (X_valid, Y_valid), (X_test, Y_test), lr=0.001, epochs=3, batch=32,\n",
    "#             seeds=[0], path='checkpoint_transformer_one_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
